{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recommendation engine for similar documents\n",
    "# Case of Literatūra un māksla (Literature and Art) \n",
    "# Literature and Art was a weekly periodical published in Latvia from 1945 to 2002."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word embedding approaches\n",
    "\n",
    "# bag of words\n",
    "# tf-idf\n",
    "# word2vec\n",
    "# doc2vec\n",
    "\n",
    "# bag of words - count vectorizer\n",
    "# tf-idf - tf-idf vectorizer\n",
    "# word2vec - word2vec model\n",
    "# doc2vec - doc2vec model\n",
    "\n",
    "# in bag of words each document is represented as a vector of token counts\n",
    "# disadvantage - the order of words is lost\n",
    "# disadvantage - the meaning of words is lost\n",
    "# disadvantage - the meaning of sentences is lost\n",
    "\n",
    "\n",
    "# in tf-idf each document is represented as a vector of token tf-idf scores\n",
    "# tf - term frequency\n",
    "# idf - inverse document frequency\n",
    "# tf-idf - term frequency times inverse document frequency\n",
    "# tf-idf is a numerical statistic that is intended to reflect how important a word is to a document in a collection or corpus\n",
    "# disadvantage - the order of words is lost\n",
    "\n",
    "# word2vec\n",
    "# word2vec was developed by Tomas Mikolov and his colleagues at Google\n",
    "# published in 2013 - relatively old approach\n",
    "# word2vec is a group of related models that are used to produce word embeddings\n",
    "# word2vec is a two-layer neural net that processes text\n",
    "# word2vec is a shallow, two-layer neural network that takes a text corpus as input and produces a set of vectors: feature vectors for words in that corpus\n",
    "# word2vec is a group of related models that are used to produce word embeddings\n",
    "# word2vec provides distributed representations for words\n",
    "# disadvantage - words can have multiple meanings, not captured by word2vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task Provide Similarity Metrics for Document Similarity\n",
    "# How to compare documents?\n",
    "# 1. Cosine Similarity\n",
    "# 2. Jaccard Similarity\n",
    "# 3. Dice Similarity\n",
    "# 4. Overlap Coefficient\n",
    "# 5. Tversky Index\n",
    "# 6. Sørensen–Dice coefficient\n",
    "# 7. Jaro–Winkler distance\n",
    "# 8. Levenshtein distance\n",
    "# 9. Hamming distance\n",
    "# 10. Ratcliff–Obershelp pattern matching\n",
    "# 11. Longest Common Subsequence\n",
    "# 12. Longest Common Substring\n",
    "# 13. Canberra distance\n",
    "\n",
    "# what is Cosine Similarity?\n",
    "# Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them\n",
    "# Cosine similarity is a measure of similarity between two non-zero vectors of an inner product space that measures the cosine of the angle between them\n",
    "# Advantage - Cosine similarity is independent of magnitude\n",
    "# Advantage - can be computed efficiently\n",
    "# Disadvantage - Cosine similarity is not a true distance metric\n",
    "# Disadvantage - Cosine similarity is not a true distance metric\n",
    "\n",
    "# Euclidean distance\n",
    "# what is Euclidean distance?\n",
    "# Euclidean distance is the \"ordinary\" straight-line distance between two points in Euclidean space\n",
    "# Advnatage - Euclidean distance is a true distance metric\n",
    "# Disadvantage - Euclidean distance is sensitive to magnitude\n",
    "\n",
    "# what is Hamming distance?\n",
    "# Hamming distance is the number of positions at which the corresponding symbols are different.\n",
    "# In other words, it measures the minimum number of substitutions required to change one string into the other, or the minimum number of errors that could have transformed one string into the other.\n",
    "# For example, the Hamming distance between:\n",
    "# 1011101 and 1001001 is 2.\n",
    "# in our case, we will use it to compare a tokenized document with a list of tokenized documents\n",
    "# for example\n",
    "# doc1 = \"I like to eat apples and bananas\"\n",
    "# doc2 = \"I like to eat apples and oranges\"\n",
    "# doc3 = \"They like to eat apples and pears\"\n",
    "\n",
    "# the distance between doc1 and doc2 is 1 because the only difference is the word \"bananas\" and \"oranges\"\n",
    "# the distance between doc1 and doc3 is 2 because the only difference is the word \"bananas\" and \"pears\"\n",
    "\n",
    "# Advantage of Hamming distance is that it can be calculated very fast\n",
    "# even in large matrices\n",
    "# matrix here is a list of tokenized documents with words as columns\n",
    "# and documents as rows\n",
    "\n",
    "# what is Levenshtein distance?\n",
    "# Levenshtein distance is a string metric for measuring the difference between two sequences.\n",
    "# Informally, the Levenshtein distance between two words is the minimum number of single-character edits (insertions, deletions or substitutions) required to change one word into the other.\n",
    "\n",
    "# what is difference between Hamming and Levenshtein distance?\n",
    "# Hamming distance is the number of positions at which the corresponding symbols are different.\n",
    "# Levenshtein distance is a string metric for measuring the difference between two sequences.\n",
    "\n",
    "# What is Canberra distance?\n",
    "# Canberra distance is a distance metric for comparing two real-valued vectors.\n",
    "# similar to Manhattan distance, but instead of using the absolute difference between the coordinates, \n",
    "# it uses the difference divided by the sum of the coordinates.\n",
    "# It is a metric in the sense that it satisfies the triangle inequality and is thus a metric space.\n",
    "# picture of canberra distance\n",
    "# https://en.wikipedia.org/wiki/Canberra_distance#/media/File:Canberra_distance.svg\n",
    "# picture of Manhattan distance\n",
    "# https://en.wikipedia.org/wiki/Taxicab_geometry#/media/File:Manhattan_distance.svg\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# deploying results\n",
    "# neo4j is a graph database - a database that stores data in the form of a graph\n",
    "# graph is a collection of nodes and edges\n",
    "# link to neo4j\n",
    "# https://neo4j.com/\n",
    "\n",
    "# in our case we will use neo4j to store the results of our similarity metrics\n",
    "# nodes will be documents\n",
    "# edges will be similarity metrics\n",
    "# for example\n",
    "# node1 = doc1\n",
    "# node2 = doc2\n",
    "# edge = canberra similarity between doc1 and doc2\n",
    "\n",
    "# advandage of neo4j is that it provides a graph visualization\n",
    "# link to neo4j graph visualization\n",
    "# https://neo4j.com/developer/guide-data-visualization/\n",
    "# also it provides a graph query language\n",
    "# link to neo4j graph query language\n",
    "# https://neo4j.com/developer/cypher-query-language/\n",
    "# somewhat similar to SQL but with a graph specific syntax\n",
    "\n",
    "# while we have matrix of ALL documents to keep graph sparse we will only use top 10 similar documents\n",
    "# this allows for discoverability of similar documents while keeping the graph small\n",
    "\n",
    "# our resulting endpoint is a REST API\n",
    "# using neo4j we can create a REST API\n",
    "# link to neo4j REST API\n",
    "# https://neo4j.com/developer/rest-api/\n",
    "\n",
    "# our neo4j community edition database is hosted locally\n",
    "# link to neo4j community edition\n",
    "# https://neo4j.com/download-center/#community\n",
    "\n",
    "# our REST API is hosted locally\n",
    "# link to neo4j REST API\n",
    "# https://tuvenieks.lnb.lv:7474\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subtask Find Partial token matches in a list of tokenized documents\n",
    "# for example\n",
    "# doc1 = \"I like to eat apples and bananas\"\n",
    "# doc2 = \"they like to throw apples and oranges\"\n",
    "\n",
    "# I want to find all matches of 5-grams with \"* like to * apples\" with * representing any token\n",
    "# the result should be\n",
    "# doc1 = \"I like to eat apples\"\n",
    "# doc2 = \"they like to throw apples\"\n",
    "# trivially this can be accomplished with a regex \"\\w* like to \\w* apples\"\n",
    "# however such search can become slow if the number of documents is large\n",
    "\n",
    "# thus some preprocessing is needed\n",
    "# one simple solution is to create a dictionary/hashtable with tokens as keys and indexes of documents as values"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
